{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\" A basic Decision Tree\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "\n",
    "    def read_data(self, filename, limit=None):\n",
    "        fid = open(filename, \"r\")\n",
    "        data = []\n",
    "        d = []\n",
    "        \n",
    "        for l,line in enumerate(fid.readlines()):\n",
    "            if limit is not None:\n",
    "                if l == limit+1:\n",
    "                    break\n",
    "            d.append(line.strip())\n",
    "            \n",
    "        for d1 in d:\n",
    "            data.append(d1.split(\",\"))\n",
    "        fid.close()\n",
    "\n",
    "        self.featureNames = data[0]\n",
    "        self.featureNames = self.featureNames[:-1]\n",
    "        data = data[1:]\n",
    "        self.classes = []\n",
    "        for d in range(len(data)):\n",
    "            self.classes.append(data[d][-1])\n",
    "            data[d] = data[d][:-1]\n",
    "\n",
    "        return data, self.classes, self.featureNames\n",
    "\n",
    "    def classify(self, tree, datapoint):\n",
    "\n",
    "        if type(tree) == type(\"string\"):\n",
    "            # Have reached a leaf\n",
    "            return tree\n",
    "        else:\n",
    "            a = list(tree.keys())[0]\n",
    "            for i in range(len(self.featureNames)):\n",
    "                if self.featureNames[i] == a:\n",
    "                    break\n",
    "\n",
    "            try:\n",
    "                t = tree[a][datapoint[i]]\n",
    "                return self.classify(t, datapoint)\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "    def classifyAll(self, tree, data):\n",
    "        results = []\n",
    "        for i in range(len(data)):\n",
    "            results.append(self.classify(tree, data[i]))\n",
    "        return results\n",
    "\n",
    "    def make_tree(self, data, classes, featureNames, maxlevel=-1, level=0, forest=0):\n",
    "        \"\"\" The main function, which recursively constructs the tree\"\"\"\n",
    "\n",
    "        nData = len(data)\n",
    "        nFeatures = len(data[0])\n",
    "\n",
    "        try:\n",
    "            self.featureNames\n",
    "        except:\n",
    "            self.featureNames = featureNames\n",
    "\n",
    "        # List the possible classes\n",
    "        newClasses = []\n",
    "        for aclass in classes:\n",
    "            if newClasses.count(aclass) == 0:\n",
    "                newClasses.append(aclass)\n",
    "\n",
    "        # Compute the default class (and total entropy)\n",
    "        frequency = np.zeros(len(newClasses))\n",
    "\n",
    "        totalEntropy = 0\n",
    "        totalGini = 0\n",
    "        index = 0\n",
    "        for aclass in newClasses:\n",
    "            frequency[index] = classes.count(aclass)\n",
    "            totalEntropy += self.calc_entropy(float(frequency[index])/nData)\n",
    "            totalGini += (float(frequency[index])/nData)**2\n",
    "\n",
    "            index += 1\n",
    "\n",
    "        totalGini = 1 - totalGini\n",
    "        default = classes[np.argmax(frequency)]\n",
    "\n",
    "        if nData == 0 or nFeatures == 0 or (maxlevel >= 0 and level > maxlevel):\n",
    "            # Have reached an empty branch\n",
    "            return default\n",
    "        elif classes.count(classes[0]) == nData:\n",
    "            # Only 1 class remains\n",
    "            return classes[0]\n",
    "        else:\n",
    "\n",
    "            # Choose which feature is best\n",
    "            gain = np.zeros(nFeatures)\n",
    "            ggain = np.zeros(nFeatures)\n",
    "            featureSet = list(range(nFeatures))\n",
    "            if forest != 0:\n",
    "                np.random.shuffle(featureSet)\n",
    "                featureSet = featureSet[0:forest]\n",
    "                \n",
    "            for feature in featureSet:\n",
    "                g, gg = self.calc_info_gain(data, classes, feature)\n",
    "                gain[feature] = totalEntropy - g\n",
    "                ggain[feature] = totalGini - gg\n",
    "\n",
    "            bestFeature = np.argmax(gain)\n",
    "            tree = {featureNames[bestFeature]: {}}\n",
    "\n",
    "            # List the values that bestFeature can take\n",
    "            values = []\n",
    "            for datapoint in data:\n",
    "                if datapoint[feature] not in values:\n",
    "                    values.append(datapoint[bestFeature])\n",
    "\n",
    "            for value in values:\n",
    "                # Find the datapoints with each feature value\n",
    "                newData = []\n",
    "                newClasses = []\n",
    "                index = 0\n",
    "                for datapoint in data:\n",
    "                    if datapoint[bestFeature] == value:\n",
    "                        if bestFeature == 0:\n",
    "                            newdatapoint = datapoint[1:]\n",
    "                            newNames = featureNames[1:]\n",
    "                        elif bestFeature == nFeatures:\n",
    "                            newdatapoint = datapoint[:-1]\n",
    "                            newNames = featureNames[:-1]\n",
    "                        else:\n",
    "                            newdatapoint = datapoint[:bestFeature]\n",
    "                            newdatapoint.extend(datapoint[bestFeature+1:])\n",
    "                            newNames = featureNames[:bestFeature]\n",
    "                            newNames.extend(featureNames[bestFeature+1:])\n",
    "                        newData.append(newdatapoint)\n",
    "                        newClasses.append(classes[index])\n",
    "                    index += 1\n",
    "\n",
    "                # Now recurse to the next level\n",
    "                subtree = self.make_tree(\n",
    "                    newData, newClasses, newNames, maxlevel, level+1, forest)\n",
    "\n",
    "                # And on returning, add the subtree on to the tree\n",
    "                tree[featureNames[bestFeature]][value] = subtree\n",
    "\n",
    "            return tree\n",
    "\n",
    "    def printTree(self, tree, name):\n",
    "        if type(tree) == dict:\n",
    "            print(name, tree.keys()[0])\n",
    "            for item in tree.values()[0].keys():\n",
    "                print (name, item)\n",
    "                self.printTree(tree.values()[0][item], name + \"\\t\")\n",
    "        else:\n",
    "            print (name, \"\\t->\\t\", tree)\n",
    "\n",
    "    def calc_entropy(self, p):\n",
    "        if p != 0:\n",
    "            return -p * np.log2(p)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def calc_info_gain(self, data, classes, feature):\n",
    "\n",
    "        # Calculates the information gain based on both entropy and the Gini impurity\n",
    "        gain = 0\n",
    "        ggain = 0\n",
    "        nData = len(data)\n",
    "\n",
    "        # List the values that feature can take\n",
    "\n",
    "        values = []\n",
    "        for datapoint in data:\n",
    "            if datapoint[feature] not in values:\n",
    "                values.append(datapoint[feature])\n",
    "\n",
    "        featureCounts = np.zeros(len(values))\n",
    "        entropy = np.zeros(len(values))\n",
    "        gini = np.zeros(len(values))\n",
    "        valueIndex = 0\n",
    "        # Find where those values appear in data[feature] and the corresponding class\n",
    "        for value in values:\n",
    "            dataIndex = 0\n",
    "            newClasses = []\n",
    "            for datapoint in data:\n",
    "                if datapoint[feature] == value:\n",
    "                    featureCounts[valueIndex] += 1\n",
    "                    newClasses.append(classes[dataIndex])\n",
    "                dataIndex += 1\n",
    "\n",
    "            # Get the values in newClasses\n",
    "            classValues = []\n",
    "            for aclass in newClasses:\n",
    "                if classValues.count(aclass) == 0:\n",
    "                    classValues.append(aclass)\n",
    "\n",
    "            classCounts = np.zeros(len(classValues))\n",
    "            classIndex = 0\n",
    "            for classValue in classValues:\n",
    "                for aclass in newClasses:\n",
    "                    if aclass == classValue:\n",
    "                        classCounts[classIndex] += 1\n",
    "                classIndex += 1\n",
    "\n",
    "            for classIndex in range(len(classValues)):\n",
    "                entropy[valueIndex] += self.calc_entropy(\n",
    "                    float(classCounts[classIndex])/np.sum(classCounts))\n",
    "                gini[valueIndex] += (float(classCounts[classIndex]\n",
    "                                           )/np.sum(classCounts))**2\n",
    "\n",
    "            # Computes both the Gini gain and the entropy\n",
    "            gain = gain + \\\n",
    "                float(featureCounts[valueIndex])/nData * entropy[valueIndex]\n",
    "            ggain = ggain + \\\n",
    "                float(featureCounts[valueIndex])/nData * gini[valueIndex]\n",
    "            valueIndex += 1\n",
    "        return gain, 1-ggain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building trees ... ( Takes a lot of time )\n",
      "1  of  10  Tree : ready after  1003  seconds\n",
      "2  of  10  Tree : ready after  329  seconds\n",
      "3  of  10  Tree : ready after  13  seconds\n",
      "4  of  10  Tree : ready after  545  seconds\n",
      "5  of  10  Tree : ready after  672  seconds\n",
      "6  of  10  Tree : ready after  7  seconds\n",
      "7  of  10  Tree : ready after  9  seconds\n",
      "8  of  10  Tree : ready after  687  seconds\n",
      "9  of  10  Tree : ready after  317  seconds\n",
      "10  of  10  Tree : ready after  524  seconds\n",
      " ... Done after  4111  seconds\n",
      "{'unacc': 293, 'None': 16, 'acc': 66}\n",
      "{'unacc': 292, 'acc': 76, 'None': 7}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def split_set(X,C, rate):\n",
    "    training_set_size = int(len(X)*rate)\n",
    "    test_set_size     = len(X) - training_set_size\n",
    "    \n",
    "    ids = []\n",
    "    \n",
    "    for i in range(test_set_size):\n",
    "        v = int(random.random()*len(X))\n",
    "        while( v in ids):\n",
    "            v = int(random.random()*len(X))\n",
    "        ids.append(v)\n",
    "    \n",
    "    train_set = []\n",
    "    train_set_answers = []\n",
    "    test_set  = []\n",
    "    test_set_answers = []\n",
    "    \n",
    "    for x in range(len(X)):\n",
    "        if x in ids:\n",
    "            test_set.append(X[x])\n",
    "            test_set_answers.append(C[x])\n",
    "        else:\n",
    "            train_set.append(X[x])\n",
    "            train_set_answers.append(C[x])\n",
    "    \n",
    "    return [train_set,train_set_answers, test_set, test_set_answers]\n",
    "\n",
    "random_forest = []\n",
    "\n",
    "def read_data(filename, limit=None):\n",
    "    fid = open(filename, \"r\")\n",
    "    data = []\n",
    "    d = []\n",
    "        \n",
    "    for l,line in enumerate(fid.readlines()):\n",
    "        if limit is not None:\n",
    "            if l == limit+1:\n",
    "                break\n",
    "        d.append(line.strip())\n",
    "            \n",
    "    for d1 in d:\n",
    "        data.append(d1.split(\",\"))\n",
    "    fid.close()\n",
    "\n",
    "    featureNames = data[0]\n",
    "    featureNames = featureNames[:-1]\n",
    "    data = data[1:]\n",
    "    classes = []\n",
    "    for d in range(len(data)):\n",
    "        classes.append(data[d][-1])\n",
    "        data[d] = data[d][:-1]\n",
    "\n",
    "    return data, classes, featureNames\n",
    "\n",
    "forestsize = 10\n",
    "\n",
    "X = read_data(\"car.data\", 1000)\n",
    "\n",
    "train_set = split_set(X[0],X[1], 0.75)\n",
    "test_set     = train_set[2]\n",
    "test_set_ans = train_set[3]\n",
    "train_set_ans= train_set[1]\n",
    "train_set    = train_set[0]\n",
    "\n",
    "v = []\n",
    "\n",
    "#print(test_set)\n",
    "#print(train_set)\n",
    "\n",
    "T = DecisionTree()\n",
    "\n",
    "print( \"Building trees ... ( Takes a lot of time )\")\n",
    "\n",
    "tim = datetime.now()\n",
    "\n",
    "for i in range(forestsize):\n",
    "    tim3 = datetime.now()\n",
    "    \n",
    "    v = split_set(train_set, train_set_ans ,0.5)\n",
    "    \n",
    "    random_forest.append( DecisionTree() )\n",
    "    \n",
    "    random_forest[i] = T.make_tree(\n",
    "        v[0],\n",
    "        v[1], \n",
    "        X[2],\n",
    "        len(X[2]),\n",
    "        forest = len(X[2])\n",
    "    )\n",
    "    \n",
    "    tim2 = datetime.now()\n",
    "\n",
    "    print(i + 1, \" of \",forestsize ,\" Tree : ready after \", (tim2-tim3).seconds, \" seconds\" )\n",
    "    \n",
    "output = {}\n",
    "output_one ={}\n",
    "\n",
    "correct     = [0,0]\n",
    "correct_one = [0,0]\n",
    "\n",
    "flag = True\n",
    "\n",
    "for j in range(len(v[2])):\n",
    "    decision = T.classify(random_forest[0],v[2][j])\n",
    "\n",
    "    if decision is not None :\n",
    "                        \n",
    "        if not decision in output_one.keys():\n",
    "            output_one[decision] = 1\n",
    "        else:\n",
    "            output_one[decision] = output_one[decision] + 1\n",
    "        if decision == v[3][j]:\n",
    "            correct_one[0] += 1\n",
    "        else :\n",
    "            correct_one[1] += 1\n",
    "\n",
    "    else:\n",
    "        if not \"None\" in output_one.keys():\n",
    "            output_one[\"None\"] = 1\n",
    "        else:\n",
    "            output_one[\"None\"] = output_one[\"None\"] + 1\n",
    "\n",
    "\n",
    "p = []\n",
    "\n",
    "for i in range(len(v[2])):\n",
    "    p.append({})\n",
    "            \n",
    "for j in range(len(v[2])):\n",
    "    for tree in random_forest:\n",
    "        decision = T.classify(tree,v[2][j])\n",
    "        if decision is None:\n",
    "            if not \"None\" in p[j].keys():\n",
    "                p[j][\"None\"] = 1\n",
    "            else:\n",
    "                p[j][\"None\"] += 1\n",
    "        else:\n",
    "            if not decision in p[j].keys():\n",
    "                p[j][decision] = 1\n",
    "            else:\n",
    "                p[j][decision] += 1\n",
    "\n",
    "#print(p)\n",
    "                \n",
    "for j in range(len(v[2])):\n",
    "    best  = -1\n",
    "    value = \"\"\n",
    "    \n",
    "    for k in p[j]:\n",
    "        if p[j][k] > best:\n",
    "            value = k\n",
    "            best  = p[j][k] \n",
    "            \n",
    "    if not value in output.keys():\n",
    "        output[value] = 1\n",
    "    else:\n",
    "        output[value] =  output[value] + 1\n",
    "        \n",
    "    if value == v[3][j]:\n",
    "        correct[0] += 1\n",
    "    else :\n",
    "        correct[1] += 1\n",
    "\n",
    "        \n",
    "print(\" ... Done after \", (tim2-tim).seconds, \" seconds\" )\n",
    "        \n",
    "print(output_one)\n",
    "#print(correct_one)       \n",
    "        \n",
    "print(output)\n",
    "#print(correct)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Tree\n",
      "   Correctly predicted :  354  out of  375\n",
      "   Accuracy :  0.944 %\n",
      "10 Trees\n",
      "   Correctly predicted :  367  out of  375\n",
      "   Accuracy :  0.9786666666666667 %\n"
     ]
    }
   ],
   "source": [
    "print(\"One Tree\")\n",
    "print(\"   Correctly predicted : \", correct_one[0], \" out of \", len(v[2]))\n",
    "print(\"   Accuracy : \", correct_one[0]/len(v[2]), \"%\")\n",
    "\n",
    "print(\"10 Trees\")\n",
    "print(\"   Correctly predicted : \", correct[0], \" out of \", len(v[2]))\n",
    "print(\"   Accuracy : \", correct[0]/len(v[2]), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
